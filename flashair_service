#!/usr/bin/python3

import argparse, time, urllib.request, sys, csv, datetime, sqlite3, sys
from os.path import expanduser, exists, basename, dirname, getsize
from os import makedirs, SEEK_END
from subprocess import run, DEVNULL, STDOUT, PIPE
from shutil import which, move
from socket import timeout as SocketTimeoutError


parser = argparse.ArgumentParser()
parser.add_argument("-f", "--flashair-ip", default="flashair.local")
#parser.add_argument("-t", "--ping-timeout", type=int, default=5)
parser.add_argument("-s", "--source-folder", default=["/DCIM", "/PRIVATE/AVCHD/BDMV/STREAM"], action='append')
parser.add_argument("-d", "--dest-folder", default=expanduser("~/FlashAir"))
parser.add_argument("-v", "--verbosity", help="increase output verbosity", action="count", default=0)
parser.add_argument("--daemon", help="Run in a continuous loop", action="store_true")
parser.add_argument("-p", "--progress", help="Print progress for file transfers", action="store_true")
parser.add_argument("-i", "--interval", type=int, help="Seconds to wait between attempts if running as daemon", default=10)
args = parser.parse_args()

# default level for non-daemon is 1
if not args.daemon:
    args.verbosity += 1

def debug(message, level=1):
    if level <= args.verbosity:
        print(message)

debug(args, 3)


tz = datetime.timezone.utc
forceUpdate = False
TIMEOUT = 5.0 # seconds
CHUNK_SIZE = 25*1024 # KB
CSV_LIST_FIELDS = ['dir', 'name', 'size', 'attr', 'date', 'time']

#sqlite transferred db
makedirs(args.dest_folder, exist_ok=True)
conn = sqlite3.connect(args.dest_folder + "/.transferred.db")
conn.execute("""CREATE TABLE IF NOT EXISTS  transferred ( 
    file TEXT NOT NULL,
    timestamp DATETIME NOT NULL,
    PRIMARY KEY (file, timestamp)
    );""")
cursor = conn.cursor()


# Find commands
jhead = which("jhead")
exiftool = which("exiftool")
fping = which("fping")
if fping:
    ping = [fping, "-r 0", args.flashair_ip]
else:
    ping = [which("ping"), "-c 1", args.flashair_ip]



def alive():
    data = run(ping, stdout=DEVNULL, stderr=STDOUT).returncode
    alive = (data == 0)
    if alive:
        debug("Ping success: {}".format(args.flashair_ip), 3)
    else:
        debug("Ping failure: {}".format(args.flashair_ip), 3)
    return alive

def faCommand(encoded_url, data=None):
    command = "http://" + args.flashair_ip + "/" + encoded_url
    debug("Sending: " + command, 2)
    try:
        reader = urllib.request.urlopen(command)
    except Exception:
        pass
        raise Exception("Command Failed")

    #return data
    data = reader.read().decode('utf-8')
    debug(data, 3)
    return data

def faTimeZone():
    try:
        data = faCommand("command.cgi?op=221")
    except Exception:
        return datetime.timezone.utc
    delta = datetime.timedelta(minutes=int(data)*15)
    return datetime.timezone(delta)


def faUsed():
    data = faCommand("command.cgi?op=140")
    sectors,bytes_per = data.split(",")
    used,total = sectors.split("/")
    return {'perc' : used/total, 'used' : used*bytes_per, 'total' : total*bytes_per}


def faUpdated():
    return faCommand("command.cgi?op=102") == "1"

def faList(dir="/"):
    data = []
    try:
        raw = faCommand("command.cgi?op=100&DIR=" + dir)
        lines = raw.splitlines()
        first = lines.pop(0)
        if first != "WLANSD_FILELIST":
            raise Exception("Invalid file list data")

        reader = csv.DictReader(lines, CSV_LIST_FIELDS)
        data = list(reader)
        return data
    except urllib.error.HTTPError as e:
        if "HTTP Error 404" in str(e):
            raise FileNotFoundError(str(e))

def parseDate(data):
    data = int(data)
    day = (data & 0b0000000000011111);
    month = (data & 0b0000000111100000) >> 5;
    year= ((data & 0b1111111000000000) >> 9) + 1980;
    return datetime.date(year, month, day)

def parseTime(data):
    data = int(data)
    seconds = (data & 0b0000000000011111) * 2;
    minutes = (data & 0b0000011111100000) >> 5;
    hours = (data & 0b1111100000000000) >> 11;
    return datetime.time(hours, minutes, seconds, 0, tz)

def tempFile(src_path):
    return "/tmp/" + src_path.lstrip("/").replace("/", "_")

def checkDownloaded(file):
    cursor.execute("SELECT file FROM transferred WHERE file=? AND timestamp=?", (file['src_path'], file['datetime'].timestamp()))
    row = cursor.fetchone()
    return row != None

def rotateFile(filename):
    if jhead:
        debug("Automatically correcting orientation", 2)
        proc = run([jhead, "-autorot", "-ft", filename], stdout=DEVNULL, stderr=STDOUT)
        debug(" ".join(proc.args), 3)
        return True
    return False

def renameFile(file):
    dest = None
    if exiftool:
        debug("Renaming file based on exif data", 2)
        proc = run([exiftool, "-p", "$DateTimeOriginal", "-d", "/%Y-%m/%Y-%m_%d-%H:%M:%S_", file['temp_file']], stdout=PIPE, stderr=DEVNULL)
        debug(format(proc), 3)
        rel_dir = proc.stdout.decode().strip()
        if rel_dir:
            dest = args.dest_folder + rel_dir + file['name']

    if not dest:
        debug("Renaming file based on file creation date", 2)
        dest = "{}/{}_{}".format(args.dest_folder, file['datetime'].strftime("%Y-%m/%Y-%m-%d_%H:%M:%S"), file['name'])
        #args.dest_folder + file['datetime'].strftime("/%Y-%m/%Y-%m-%d_%H:%M:%S_") + file['name']

    makedirs(dirname(dest), exist_ok=True)
    move(file['temp_file'], dest)
    return dest

def print_progress(ratio):
    count = int(ratio * 20)
    sys.stdout.write('\r')
    sys.stdout.write("[{:<20}] {:.2f}%".format('='*count, 100*ratio))
    sys.stdout.flush()


def transferFile(file):
    src = "http://" + args.flashair_ip + file['src_path']
    tmp = file['temp_file']
    file_size = int(file['size'])
    request = urllib.request.Request(src)

    bytes_read = 0
    if exists(tmp):
        # resume transfer with an http range request
        bytes_read = getsize(tmp)
        request.add_header('Range', 'bytes={}-'.format(bytes_read))

    # The file might be complete, skip downloading it if it is
    if bytes_read < file_size:
        stream = urllib.request.urlopen(request, timeout=TIMEOUT)
        with open(tmp, 'ab') as f:
            while(bytes_read < file_size):
                chunk = stream.read(CHUNK_SIZE)
                chunk_len = len(chunk)
                if chunk_len == CHUNK_SIZE or bytes_read + chunk_len == file_size:
                    bytes_read += len(chunk)
                    f.write(chunk)
                    if args.progress:
                        print_progress(bytes_read / file_size)
                else:
                    raise IOError("Partial chunk")

            if args.progress:
                print() # print line return after progress bar

    debug("Inserting into transferred db: {} {:.0f}".format(file['src_path'], file['datetime'].timestamp()), 2)
    cursor.execute("INSERT INTO transferred values(?,?)", (file['src_path'], int(file['datetime'].timestamp())))
    conn.commit() # make sure the db is saved in case we get interrupted
    debug("Inserted: {}".format(cursor.lastrowid), 3)

def syncFile(file):
    global forceUpdate
    try:
        if not checkDownloaded(file):
            debug(file, 3)
            print("Transferring file: {}".format(file['src_path']))
            transferFile(file)
            rotateFile(file['temp_file'])
            file['dest_path'] = renameFile(file)
            print("Saved to: {}".format(file['dest_path']))
                
    except (SocketTimeoutError, IOError) as e:
        if args.progress:
            print() # print line return after progress bar
        print("Transfer interrupted: {}".format(e))
        forceUpdate = True

def syncDir(dir):
    files = faList(dir)
    debug(files, 4)
    
    for file in files:
        file['src_path'] = file['dir'].rstrip("/") + "/" + file['name']
        if int(file['attr']) & 16:
            # bit 5 = dir, recurse into dir
            #dst_dir = args.dest_folder + "/" + file['src_path'].strip("/")
            #debug("Making directory: " + dst_dir, 3)
            #makedirs(dst_dir, exist_ok=True)
            syncDir(file['src_path'])
            continue
        file['temp_file'] = tempFile(file['src_path'])
        file['datetime'] = datetime.datetime.combine(parseDate(file['date']), parseTime(file['time']))

        syncFile(file)


print("Starting " + basename(__file__) + "...")
print("Syncing " + str(args.source_folder) + " from " + args.flashair_ip)

while True:
    if alive():
        global forceUpdate
        debug("FlashAir card found!")
        if forceUpdate or faUpdated():
            forceUpdate = False
            tz = faTimeZone()

            for folder in args.source_folder:
                syncDir(folder)

    elif not args.daemon:
        # don't spam the logs when daemonized, but be friendly to humans
        print("Could not reach FlashAir card: " + args.flashair_ip)


    # only run once by default, if --daemon loop
    if not args.daemon:
        break

    # wait so we don't monopolize the CPU
    time.sleep(args.interval)


# save sqlite and close
conn.commit()
conn.close()

